{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 12:11:31.120732: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-22 12:11:31.120969: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import import_ipynb\n",
    "import Setting\n",
    "\n",
    "\n",
    "### import the libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_docs as tfdocs\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "def percentage_error(actual, predicted):\n",
    "    res = numpy.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return numpy.mean(numpy.abs(percentage_error(numpy.asarray(y_true), numpy.asarray(y_pred)))) * 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposed_method(datass,look_back):\n",
    "\n",
    "    from PyEMD import CEEMDAN\n",
    "\n",
    "\n",
    "    dfs=datass\n",
    "    s = dfs.values\n",
    "\n",
    "    emd = CEEMDAN(epsilon=0.05)\n",
    "    emd.noise_seed(12345)\n",
    "\n",
    "    IMFs = emd(s)\n",
    "\n",
    "\n",
    "    full_imf=pd.DataFrame(IMFs)\n",
    "    data_imf=full_imf.T\n",
    "\n",
    "\n",
    "\n",
    "    pred_test=[]\n",
    "    test_ori=[]\n",
    "    pred_train=[]\n",
    "    train_ori=[]\n",
    "\n",
    "     \n",
    "    n_imf=len(data_imf.columns)\n",
    "\n",
    "    k=list(range(1,n_imf))\n",
    "    m=[0]\n",
    "\n",
    "\n",
    "    for i in m:  \n",
    "        \n",
    "        datasetss2=pd.DataFrame(data_imf[i])\n",
    "        datasets=datasetss2.values\n",
    "        train_size = int(len(datasets) * Setting.data_partition)\n",
    "        test_size = len(datasets) - train_size\n",
    "        train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
    "\n",
    "        trainX, trainY = create_dataset(train, look_back)\n",
    "        testX, testY = create_dataset(test, look_back)\n",
    "        X_train=pd.DataFrame(trainX)\n",
    "        Y_train=pd.DataFrame(trainY)\n",
    "        X_test=pd.DataFrame(testX)\n",
    "        Y_test=pd.DataFrame(testY)\n",
    "        sc_X = StandardScaler()\n",
    "        sc_y = StandardScaler()\n",
    "        X= sc_X.fit_transform(X_train)\n",
    "        y= sc_y.fit_transform(Y_train)\n",
    "        X1= sc_X.fit_transform(X_test)\n",
    "        y1= sc_y.fit_transform(Y_test)\n",
    "        y=y.ravel()\n",
    "        y1=y1.ravel()  \n",
    "        \n",
    "        import numpy\n",
    "        \n",
    "        trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
    "        testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
    "\n",
    "        numpy.random.seed(1234)\n",
    "        import tensorflow as tf\n",
    "        tf.random.set_seed(1234)\n",
    "        \n",
    "\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "        grid = RandomForestRegressor(max_features=Setting.max_features)\n",
    "        grid.fit(X,y)\n",
    "        y_pred_train= grid.predict(X)\n",
    "        y_pred_test= grid.predict(X1)\n",
    "\n",
    "        y_pred_test=pd.DataFrame(y_pred_test)\n",
    "        y_pred_train=pd.DataFrame(y_pred_train)\n",
    "\n",
    "        y1=pd.DataFrame(y1)\n",
    "        y=pd.DataFrame(y)\n",
    "       \n",
    "        y_test= sc_y.inverse_transform (y1)\n",
    "        y_train= sc_y.inverse_transform (y)\n",
    "\n",
    "        y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
    "        y_pred_train1= sc_y.inverse_transform (y_pred_train)\n",
    "\n",
    "\n",
    "        pred_test.append(y_pred_test1)\n",
    "        test_ori.append(y_test)\n",
    "        pred_train.append(y_pred_train1)\n",
    "        train_ori.append(y_train)\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    for i in k:  \n",
    "        \n",
    "        datasetss2=pd.DataFrame(data_imf[i])\n",
    "        datasets=datasetss2.values\n",
    "        train_size = int(len(datasets) * Setting.data_partition)\n",
    "        test_size = len(datasets) - train_size\n",
    "        train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
    "\n",
    "        trainX, trainY = create_dataset(train, look_back)\n",
    "        testX, testY = create_dataset(test, look_back)\n",
    "        X_train=pd.DataFrame(trainX)\n",
    "        Y_train=pd.DataFrame(trainY)\n",
    "        X_test=pd.DataFrame(testX)\n",
    "        Y_test=pd.DataFrame(testY)\n",
    "        sc_X = StandardScaler()\n",
    "        sc_y = StandardScaler()\n",
    "        X= sc_X.fit_transform(X_train)\n",
    "        y= sc_y.fit_transform(Y_train)\n",
    "        X1= sc_X.fit_transform(X_test)\n",
    "        y1= sc_y.fit_transform(Y_test)\n",
    "        y=y.ravel()\n",
    "        y1=y1.ravel()  \n",
    "        \n",
    "        import numpy\n",
    "        \n",
    "        trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
    "        testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
    "\n",
    "        numpy.random.seed(1234)\n",
    "        import tensorflow as tf\n",
    "        tf.random.set_seed(1234)\n",
    "\n",
    "\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers.core import Dense, Dropout, Activation\n",
    "        from keras.layers.recurrent import LSTM\n",
    "\n",
    "        neuron=Setting.neuron\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units = neuron,input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "        model.add(Dense(1))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=Setting.lr)\n",
    "        model.compile(loss='mse',optimizer=Setting.optimizer)\n",
    "\n",
    "\n",
    "        numpy.random.seed(1234)\n",
    "\n",
    "        \n",
    "     # Fitting the RNN to the Training set\n",
    "        model.fit(trainX, y, epochs = Setting.epoch, batch_size = Setting.batch_size,verbose=0)\n",
    "\n",
    "\n",
    "        # make predictions\n",
    "        y_pred_train = model.predict(trainX)\n",
    "        y_pred_test = model.predict(testX)\n",
    "    \n",
    "        # make predictions\n",
    "    \n",
    "\n",
    "        y_pred_test= numpy.array(y_pred_test).ravel()\n",
    "        y_pred_test=pd.DataFrame(y_pred_test)\n",
    "        y1=pd.DataFrame(y1)\n",
    "        y=pd.DataFrame(y)\n",
    "        y_pred_train= numpy.array(y_pred_train).ravel()\n",
    "        y_pred_train=pd.DataFrame(y_pred_train)\n",
    "              \n",
    "        \n",
    "        y_test= sc_y.inverse_transform (y1)\n",
    "        y_train= sc_y.inverse_transform (y)\n",
    "\n",
    "        y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
    "        y_pred_train1= sc_y.inverse_transform (y_pred_train)\n",
    "\n",
    "\n",
    "        pred_test.append(y_pred_test1)\n",
    "        test_ori.append(y_test)\n",
    "        pred_train.append(y_pred_train1)\n",
    "        train_ori.append(y_train)\n",
    "\n",
    "    result_pred_test= pd.DataFrame.from_records(pred_test)\n",
    "    result_pred_train= pd.DataFrame.from_records(pred_train)\n",
    "\n",
    "\n",
    "    a=result_pred_test.sum(axis = 0, skipna = True) \n",
    "    b=result_pred_train.sum(axis = 0, skipna = True) \n",
    "\n",
    "\n",
    "    dataframe=pd.DataFrame(dfs)\n",
    "    dataset=dataframe.values\n",
    "\n",
    "    train_size = int(len(dataset) * Setting.data_partition)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
    "\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    X_train=pd.DataFrame(trainX)\n",
    "    Y_train=pd.DataFrame(trainY)\n",
    "    X_test=pd.DataFrame(testX)\n",
    "    Y_test=pd.DataFrame(testY)\n",
    "\n",
    "    sc_X = StandardScaler()\n",
    "    sc_y = StandardScaler() \n",
    "    X= sc_X.fit_transform(X_train)\n",
    "    y= sc_y.fit_transform(Y_train)\n",
    "    X1= sc_X.fit_transform(X_test)\n",
    "    y1= sc_y.fit_transform(Y_test)\n",
    "    y=y.ravel()\n",
    "    y1=y1.ravel()\n",
    "\n",
    "\n",
    "    trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
    "    testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
    "\n",
    "    numpy.random.seed(1234)\n",
    "    import tensorflow as tf\n",
    "    tf.random.set_seed(1234)\n",
    "\n",
    "    y1=pd.DataFrame(y1)\n",
    "    y=pd.DataFrame(y)\n",
    "    y_test= sc_y.inverse_transform (y1)\n",
    "    y_train= sc_y.inverse_transform (y)\n",
    "\n",
    "    \n",
    "    a= pd.DataFrame(a)    \n",
    "    y_test= pd.DataFrame(y_test)    \n",
    "    \n",
    "\n",
    "    import numpy as np\n",
    "    \n",
    "\n",
    "   #summarize the fit of the model\n",
    "    mape=mean_absolute_percentage_error(y_test,a)\n",
    "    rmse= sqrt(mean_squared_error(y_test,a))\n",
    "    mae=metrics.mean_absolute_error(y_test,a)\n",
    "    \n",
    "    return mape,rmse,mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
